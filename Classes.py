# -*- coding: utf-8 -*-
"""Classes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Sahar-Zi/Medical-Speech/blob/main/Classes.ipynb

#**EDA and Modeling Classes**
##**EDAClass**
The EDAClass is designed for Exploratory Data Analysis (EDA) and feature engineering on datasets involving audio files and associated metadata. This class provides methods to preprocess data, perform various EDA tasks, extract and analyze audio features, and conduct clustering analysis.

**Constructor**:

* `__init__(self, filepath, audio_dir, binary_outcome=True)`: Initializes the class with a CSV file containing metadata and a directory for audio files. Optionally subsets the data to include only specified categories if binary_outcome is set to True.

**Methods**:

* `get_data()`: Returns the loaded dataset as a pandas DataFrame.

* `EDA_quality_measures()`: Plots frequency, histogram, and density plots for quality measures in the dataset.

* `EDA_writers_speakers()`: Provides histograms for speaker and writer IDs.

* `EDA_symptoms()`: Plots label frequencies for symptoms.

* `Correlation_Matrix()`: Displays a heatmap of the absolute correlations between features.

* `NLP(show_wordcloud=True, drop_leading_words=False, inplace=False)`: Performs Natural Language Processing on the 'phrase' column to clean and preprocess text, with options to display a word cloud and/or remove leading words. Updates features in the dataset or returns them based on the inplace parameter.

* `Add_audio_features(inplace=False)`: Extracts and adds audio features from audio files to the dataset. Features include MFCCs, spectral centroid, zero crossing rate, and root mean square energy.

* `Clustering(random_state=1, n_components=5, n_clusters=2, inplace=False)`: Applies clustering algorithms (KMeans and Gaussian Mixture Models) to the features using PCA and t-SNE for dimensionality reduction. Optionally updates the dataset with cluster labels.

---
##**modelingClass**
The modelingClass is designed to facilitate the machine learning modeling process, encompassing data preparation, model training, testing, and result visualization. This class provides methods to split and scale data, train various machine learning models using grid search for hyperparameter tuning, evaluate model performance, and visualize results including decision tree plots and confusion matrices.

**Constructor**:

* `__init__(self, X, y, NLP=False)`: Initializes the class with feature dataset X and target labels y. Optionally specifies whether the data pertains to Natural Language Processing (NLP) tasks through the NLP parameter.

**Methods**:

* `split_scale_data(random_state=1, test_size=0.3)`: Splits the dataset into training and test sets and scales the features. Handles exceptions related to data format and consistency.

* `train_models(random_state=1, cv=5)`: Trains various machine learning models (Logistic Regression, K-Nearest Neighbors, Support Vector Machine, Decision Tree) using GridSearchCV to find the best hyperparameters. Handles different setups for NLP and non-NLP tasks.

* `test_models()`: Tests the trained models on the test dataset and stores predictions. Ensures that models are trained before testing.

* `plot_tree()`: Plots the decision tree model if it has been trained. Raises an error for NLP models as decision trees are not applicable.

* `results(show_df=False, show_df_in_latex=False, show_cm=False, show_eval=False)`: Displays the results including confusion matrices and evaluation metrics. Offers options to display results as a DataFrame or in LaTeX format, show confusion matrices, and plot evaluation metrics.

---

# **Table of Contents**

---

#### 1. [Libraries Imports](#SectionImports)

#### 2. [Explenatory Data Analysis Class](#SectionEDA)

#### 3. [Modeling Class](#SectionModeling)

---

# **Libraries Imports**
<a class="anchor" id="SectionImports"></a>
"""

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, Latex

# Text
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string
from wordcloud import WordCloud
from collections import Counter
from nltk.corpus import wordnet
import re

# Audio
import librosa
import os
import IPython

# Clustering
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

# Modeling
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler

"""# **EDA and Feature Engineering**
<a class="anchor" id="SectionEDA"></a>
"""

class EDAClass:
  def __init__(self, filepath, audio_dir, binary_outcome = True):
    """
    Initializes the EDAObject with the given file path and audio directory.

    Args:
      filepath (str): Path to the CSV file containing the data.
      audio_dir (str): Directory where audio files are stored.
      binary_outcome (bool): If True, subset the data to only include "Cough" and "Infected wound".
    """
    self.audio_dir = audio_dir
    self.data = pd.read_csv(filepath)

    self.data.rename(columns={'prompt':'Symptoms'}, inplace=True)
    # Subset for binary outcome: "Cough" or "Infected Wound"
    if binary_outcome:
      self.data = self.data[((self.data['Symptoms'] == "Cough") | (self.data['Symptoms'] == "Infected wound"))]
      self.data.reset_index(drop=True, inplace=True)

    # Find and convert features to category type
    category_features = self.data.select_dtypes(include=['object']).columns.append(self.data.select_dtypes(include=['int64']).columns)
    self.data[category_features] = self.data[category_features].astype('category')
    self.data['phrase'] = self.data['phrase'].astype('string')
    self.data['file_name'] = self.data['file_name'].astype('string')

    self.X = self.data.select_dtypes(include=['float64'])
    self.y = self.data['Symptoms']

  def get_data(self):
    """
    Returns the data loaded into the EDAObject.

    Returns:
      pd.DataFrame: DataFrame containing the data.
    """
    return self.data

  def EDA_quality_measures(self):
    """
    Performs Exploratory Data Analysis (EDA) on quality measures.
    Plots frequency, histogram, and density plots for audio quality measures.
    """
    quality_measures = ['audio_clipping', 'quiet_speaker', 'background_noise_audible']

    fig, axes = plt.subplots(3, len(quality_measures), figsize=(18, 12), gridspec_kw={'height_ratios': [0.6, 3, 3]})

    for i, column in enumerate(quality_measures):
        sns.heatmap(self.data[column].value_counts().to_frame().T,
                    ax=axes[0, i], annot=True, fmt="d", cmap="Set3", cbar=False, yticklabels=False)
        axes[0, i].xaxis.tick_top()
        axes[0, i].xaxis.set_label_position('top')
        axes[0, i].set_title(column.capitalize(), fontsize=16)
        axes[0, i].set(xlabel="")
        sns.histplot(data=self.data, ax=axes[1, i], x=column+":confidence", fill=True)
        axes[1, i].set_xlim(0, 1.2)
        axes[1, i].set_ylim(0, 600)
        sns.kdeplot(data=self.data, ax=axes[2, i], x=column+":confidence", hue=column, palette="tab10", fill=True, warn_singular=False)
        axes[2, i].set_xlim(0, 1.2)
        sns.move_legend(axes[2, i], "upper left")

    fig.suptitle("Frequency, Histogram, and Density of Quality Measures", y=0.97, fontsize=20)
    plt.show()

    fig, axes = plt.subplots(1, 2, figsize=(18, 5))

    sns.histplot(data=self.data, ax=axes[0], x="overall_quality_of_the_audio", fill=True, bins=6)
    sns.kdeplot(data=self.data, ax=axes[1], x="overall_quality_of_the_audio", hue="Symptoms", fill=True, warn_singular=False)
    axes[0].set_xlim(0, 5)
    axes[1].set_xlim(0, 5)
    sns.move_legend(axes[1], "upper left")

    fig.suptitle("Histogram, and Density of the Overall Quality Measure", y=0.97, fontsize=20)
    plt.show()

  def EDA_writers_speakers(self):
    """
    Performs Exploratory Data Analysis (EDA) on writer and speaker IDs.
    Plots histograms for speaker and writer IDs.
    """
    plt.figure(figsize=(20, 6))
    self.data['speaker_id'].value_counts().plot(kind='bar')
    plt.title('Speaker ID Histogram')
    plt.ylabel('Count')
    plt.show()

    plt.figure(figsize=(10, 4))
    self.data['writer_id'].value_counts().plot(kind='bar')
    plt.title('Writer ID Histogram')
    plt.ylabel('Count')
    plt.show()

  def EDA_symptoms(self):
    """
    Performs Exploratory Data Analysis (EDA) on symptoms.
    Plots label frequencies of symptoms.
    """
    x = self.data['Symptoms'].nunique()
    plt.figure(figsize=(0.34782608695652*x + 3.304347826087, 0.17391304347826*x + 1.6521739130435))
    ax = sns.countplot(y="Symptoms", data=self.data, order=self.data['Symptoms'].value_counts().index)
    plt.title("Label Frequencies")
    for p in ax.patches:
      ax.annotate(int(p.get_width()), (p.get_width()+1.2608695652174*x - 42.521739130435, p.get_y()+0.0047826086956522*x + 0.4404347826087),color="white",fontsize=-0.17391304347826*x + 12.347826086957)
    plt.show()

  def Correlation_Matrix(self):
    """
    Plots the absolute correlation heatmap for the features.
    """
    plt.figure(figsize=(self.X.shape[1], self.X.shape[1]))
    sns.heatmap(self.X.corr().abs(), cmap='YlOrRd', annot=True)
    plt.title('Absolute Correlation Heatmap')
    plt.show()

  def NLP(self, show_wordcloud = True, drop_leading_words = False, inplace = False):
    """
    Performs NLP preprocessing on the 'phrase' column.
    Optionally displays a word cloud and drops leading words.

    Args:
      show_wordcloud (bool): If True, displays a word cloud.
      drop_leading_words (bool): If True, drops leading words related to symptoms.
      inplace (bool): If True, updates the X attribute of the class with new features.
    """
    def abv_decontraction(phrase):
      # specific
      phrase = re.sub(r"won\'t", "will not", phrase)
      phrase = re.sub(r"can\'t", "can not", phrase)

      # general
      phrase = re.sub(r"n\'t", " not", phrase)
      phrase = re.sub(r"\'re", " are", phrase)
      phrase = re.sub(r"\'s", " is", phrase)
      phrase = re.sub(r"\'d", " would", phrase)
      phrase = re.sub(r"\'ll", " will", phrase)
      phrase = re.sub(r"\'t", " not", phrase)
      phrase = re.sub(r"\'ve", " have", phrase)
      phrase = re.sub(r"\'m", " am", phrase)
      return phrase

    # Abbreviations deconstraction
    phrases = self.data['phrase'].apply(lambda x: abv_decontraction(x))

    # Lower case all letters
    phrases = phrases.apply(lambda x:x.lower())

    # Remove Special characters
    phrases = phrases.str.replace('\W', ' ')

    # Remove stop words
    STOPWORDS = stopwords.words('english')
    phrases = phrases.apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS]))

    # Lemmatization
    lem = WordNetLemmatizer()
    phrases = phrases.apply(lambda x: ' '.join([lem.lemmatize(word) for word in x.split()]))

    # Remove special charachters
    alphabet = list(string.ascii_lowercase)
    phrases = phrases.apply(lambda x: ' '.join([word for word in x.split() if all(map(lambda letter: letter in alphabet, word))]))

    if show_wordcloud:
      word_dict = dict(Counter([word for phrase in phrases for word in phrase.split()]))
      popular_words = sorted(word_dict, key = word_dict.get, reverse = True)
      word_string=" ".join(popular_words)
      wordcloud = WordCloud(background_color='white',
                            colormap='plasma',
                            max_words=len(word_dict),
                            width=1920,height=1080).generate(word_string)
      plt.clf()
      plt.imshow(wordcloud)
      plt.axis('off')
      plt.show()

      phrases_df = phrases.str.get_dummies(sep=' ')

      if drop_leading_words:
        phrases_df.drop(['cough', 'coughing','infected','infection','wound'], axis=1, inplace = True)

      if inplace:
        self.X = self.X.join(phrases_df)
      else:
        return self.X.join(phrases_df)

  def Add_audio_features(self, inplace = False):
    """
    Adds audio features extracted from audio files.

    Args:
      inplace (bool): If True, updates the X attribute of the class with new features.
    """
    def extract_features(filename):
      """
      Extracts audio features from a given audio file.

      Args:
        filename (str): Name of the audio file.

      Returns:
        dict: Dictionary of extracted audio features.
      """
      # Load audio file
      file_path = os.path.join(self.audio_dir, filename)
      y, sr = librosa.load(file_path)
      duration = librosa.get_duration(y=y, sr=sr)
      mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
      mfcc_means = np.mean(mfccs, axis=1)
      mfcc_stds = np.std(mfccs, axis=1)
      spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
      zero_crossing_rate = librosa.feature.zero_crossing_rate(y)
      rmse = librosa.feature.rms(y=y)

      # Aggregate features
      features = {
          'duration': duration,
          'spectral_centroid_mean': np.mean(spectral_centroid),
          'spectral_centroid_std': np.std(spectral_centroid),
          'zero_crossing_rate_mean': np.mean(zero_crossing_rate),
          'zero_crossing_rate_std': np.std(zero_crossing_rate),
          'rms_energy_mean': np.mean(rmse),
          'rms_energy_std': np.std(rmse)
      }
      features.update({f'mfcc_{i+1}': mfcc_means[i]/mfcc_stds[i] for i in range(13)})
      return features

    audio_features = pd.DataFrame(self.data['file_name'].apply(extract_features).tolist())

    if inplace:
      self.X = self.X.join(audio_features)
    else:
      return self.X.join(audio_features)

  def Clustering(self, random_state = 1, n_components = 5, n_clusters = 2, inplace = False):
    """
    Performs clustering on the features using PCA, t-SNE, KMeans, and Gaussian Mixture Models.

    Args:
      random_state (int): Random state for reproducibility.
      n_components (int): Number of components for PCA and t-SNE.
      n_clusters (int): Number of clusters for KMeans and GMM.
      inplace (bool): If True, updates the X attribute of the class with cluster labels.
    """
    X_scaled = pd.DataFrame(StandardScaler().fit_transform(self.X), columns=self.X.columns)
    PCA_clustered_data = pd.DataFrame(PCA(n_components=n_components).fit_transform(X_scaled), columns=[f'PC {i+1}' for i in range(n_components)])
    TSNE_clustered_data = pd.DataFrame(TSNE(n_components=n_components, random_state=random_state, init='random', method="exact", perplexity=2).fit_transform(X_scaled), columns=[f'TSNE {i+1}' for i in range(n_components)])

    km_method = pd.Series(KMeans(random_state = random_state, n_clusters = n_clusters,n_init='auto').fit(X_scaled).labels_)
    PCA_clustered_data['clusters_km'] = km_method
    TSNE_clustered_data['clusters_km'] = km_method

    gmm_method = pd.Series(GaussianMixture(random_state = random_state, n_components = n_clusters).fit_predict(X_scaled))
    PCA_clustered_data['clusters_gmm'] = gmm_method
    TSNE_clustered_data['clusters_gmm'] = gmm_method

    fig, axes = plt.subplots(2, 2, figsize=(18, 10))
    sns.scatterplot(x=PCA_clustered_data['PC 1'], y=PCA_clustered_data['PC 2'], hue=PCA_clustered_data['clusters_km'], s=100, ax=axes[0,0])
    sns.scatterplot(x=TSNE_clustered_data['TSNE 1'], y=TSNE_clustered_data['TSNE 2'], hue=TSNE_clustered_data['clusters_km'], s=100, ax=axes[0,1])
    sns.scatterplot(x=PCA_clustered_data['PC 1'], y=PCA_clustered_data['PC 2'], hue=PCA_clustered_data['clusters_gmm'], s=100, ax=axes[1,0])
    sns.scatterplot(x=TSNE_clustered_data['TSNE 1'], y=TSNE_clustered_data['TSNE 2'], hue=TSNE_clustered_data['clusters_gmm'], s=100, ax=axes[1,1])
    axes[0,0].set_title('PC 1 against PC 2 for K-means')
    axes[0,1].set_title('TSNE 1 against TSNE 2 for K-means')
    axes[1,0].set_title('PC 1 against PC 2 for Gaussian Mixture Model')
    axes[1,1].set_title('TSNE 1 against TSNE 2 for Gaussian Mixture Model')
    axes[0,0].grid()
    axes[0,1].grid()
    axes[1,0].grid()
    axes[1,1].grid()
    plt.show()

    if inplace:
      self.X = self.X.join(PCA_clustered_data['clusters_gmm'])
    else:
      return self.X.join(PCA_clustered_data['clusters_gmm'])

"""# **Modeling**
<a class="anchor" id="SectionModeling"></a>
"""

class modelingClass:
  """
  A class to encapsulate the machine learning modeling process including data splitting, scaling,
  training various models, testing, and visualizing results.

  Attributes:
      X (DataFrame or ndarray): Features dataset.
      y (Series or ndarray): Target labels.
      NLP (bool): Whether the data is for NLP tasks.
      X_train (ndarray): Training set features.
      X_test (ndarray): Test set features.
      X_train_unscaled (ndarray): Unscaled training set features.
      X_test_unscaled (ndarray): Unscaled test set features.
      y_train (ndarray): Training set labels.
      y_test (ndarray): Test set labels.
      best_model (dict): Dictionary to store the best model for each algorithm.
      best_params (dict): Dictionary to store the best parameters for each model.
      predictions (dict): Dictionary to store predictions for each model.
  """
  def __init__(self, X, y, NLP = False):
    """
    Initializes the modelingObject with the dataset and target labels.

    Args:
        X (DataFrame or ndarray): Features dataset.
        y (Series or ndarray): Target labels.
        NLP (bool): Whether the data is for NLP tasks. Defaults to False.
    """
    self.X = X
    if type(y) == pd.core.series.Series:
      self.y = y.values
    else:
      self.y = y
    self.NLP = NLP
    self.X_train = None
    self.X_test = None
    self.X_train_unscaled = None
    self.X_test_unscaled = None
    self.y_train = None
    self.y_test = None
    self.best_model = {}
    self.best_params = {}
    self.predictions = {}

  def split_scale_data(self, random_state=1, test_size=0.3):
    """
    Splits the dataset into training and test sets and scales the features.

    Args:
        random_state (int): Random state for reproducibility. Defaults to 1.
        test_size (float): Proportion of the dataset to include in the test split. Defaults to 0.3.
    """
    # Execptions
    if self.X is None or (type(self.X) != pd.core.frame.DataFrame and type(self.X) != np.ndarray):
      raise ValueError("X must be a pandas DataFrame or numpy array")
    if self.y is None or (type(self.y) != pd.core.series.Series and type(self.y) != np.ndarray):
      raise ValueError("y must be a pandas Series or numpy array")
    if len(self.y.shape) != 1 :
      raise ValueError("y must have only one column")
    if self.X.shape[0] != self.y.shape[0]:
      raise ValueError("X and y must have the same number of rows")
    if self.X.shape[0] < 2 or y.shape[0] < 2:
      raise ValueError("X and y must have at least 2 rows")

    # Split the data into train and test sets and scale it
    print("Splitting and scaling the data")
    print("-----------------------------------")
    if self.X_train is None:
      self.X_train_unscaled, self.X_test_unscaled, self.y_train, self. y_test = train_test_split(self.X, self.y, random_state=random_state, test_size=test_size)
      scaler = StandardScaler().fit(self.X_train_unscaled)
      self.X_train = scaler.transform(self.X_train_unscaled)
      self.X_test = scaler.transform(self.X_test_unscaled)


  def train_models(self, random_state=1, cv=5):
    """
    Trains various machine learning models using GridSearchCV to find the best parameters.

    Args:
        random_state (int): Random state for reproducibility. Defaults to 1.
        cv (int): Number of cross-validation folds. Defaults to 5.
    """
    # Exceptions
    if self.X_train is None:
      self.split_scale_data(random_state = random_state)

    # Initialization of the models
    init_LR = LogisticRegression(random_state=random_state, solver = 'saga')  # For logistic regression with regularization one must choose 'saga' solver
    if not self.NLP:
      init_KNN = KNeighborsClassifier()
      init_SVM = SVC(random_state=random_state, probability=True)
      init_DT = DecisionTreeClassifier(random_state=random_state)

    # Setting grid search parameter grids'
    parameters_LR = {'penalty':['l1','l2'], # L1 penalty = Lasso, L2 penalty = Ridge
                      'C': np.exp(np.arange(-1,0,0.05))} # small C values means stronger regularization and the opposite
    if not self.NLP:
      parameters_KNN = {'n_neighbors': range(3, self.X_test.shape[0], 2)}
      parameters_SVM = {'C': np.exp(np.arange(-1,0,0.1)),
                        'kernel': ['linear', 'poly', 'rbf'],
                        'degree': np.arange(3,6,1),
                        'gamma': 10**np.arange(-4, 0, 1, dtype=np.float64)}
      parameters_DT = {'max_depth': np.arange(3,7,1),
                        'min_samples_leaf': np.arange(5,8,1),
                        'criterion': ['gini','entropy']}

    # Cross validation and finding the most 'accurate' model for each set of parameters
    print("Training Phase:")
    print("Training \'Logistic Regression\' model")
    GSCV = GridSearchCV(init_LR, parameters_LR, cv=cv, n_jobs=-1, scoring='accuracy', verbose=3).fit(self.X_train, self.y_train)
    self.best_model["Logistic Regression"] = GSCV.best_estimator_
    self.best_params["Logistic Regression"] = GSCV.best_params_
    if not self.NLP:
      print("Training \'K-Nearest Neighbors\' model")
      GSCV = GridSearchCV(init_KNN, parameters_KNN, cv=cv, n_jobs=-1, scoring='accuracy', verbose=3).fit(self.X_train, self.y_train)
      self.best_model["K-Nearest Neighbors"] = GSCV.best_estimator_
      self.best_params["K-Nearest Neighbors"] = GSCV.best_params_
      print("Training \'Support Vector Machine\' model")
      GSCV = GridSearchCV(init_SVM, parameters_SVM, cv=cv, n_jobs=-1, verbose=3).fit(self.X_train, self.y_train)
      self.best_model["Support Vector Machine"] = GSCV.best_estimator_
      self.best_params["Support Vector Machine"] = GSCV.best_params_
      print("Training \'Decision Tree\' model")
      GSCV = GridSearchCV(init_DT, parameters_DT, cv=cv, n_jobs=-1, scoring='accuracy', verbose=3).fit(self.X_train_unscaled, self.y_train)
      self.best_model["Decision Tree"] = GSCV.best_estimator_
      self.best_params["Decision Tree"] = GSCV.best_params_
    print("The training of the models is done.")
    print("-----------------------------------")

  def test_models(self):
    """
    Tests the trained models on the test dataset and stores the predictions.
    """
    # Exceptions
    if not self.best_model:
      self.train_models()

    # Testing the models
    print("Testing Phase:")
    print("Testing \'Logistic Regression\' model")
    self.predictions["Logistic Regression"] = self.best_model["Logistic Regression"].predict(self.X_test)
    if not self.NLP:
      print("Testing \'K-Nearest Neighbors\' model")
      self.predictions["K-Nearest Neighbors"] = self.best_model["K-Nearest Neighbors"].predict(self.X_test)
      print("Testing \'Support Vector Machine\' model")
      self.predictions["Support Vector Machine"] = self.best_model["Support Vector Machine"].predict(self.X_test)
      print("Testing \'Decision Tree\' model")
      self.predictions["Decision Tree"] = self.best_model["Decision Tree"].predict(self.X_test_unscaled)
    print("The testing of the models is done.")
    print("-----------------------------------")

  def plot_tree(self):
    """
    Plots the decision tree model if it's included in the trained models.
    """
    # Execptions
    if self.NLP:
      raise ValueError("This method is not for NLP models")
    if not self.predictions:
      self.test_models()

    # Plot the decision tree fitted over the train set
    d = self.best_model["Decision Tree"].get_depth()
    plt.figure(figsize=(6*(d+1),3*(d+1)))
    tree.plot_tree(self.best_model["Decision Tree"].fit(self.X_train_unscaled, self.y_train), fontsize=15, filled=True, feature_names=self.X.columns, impurity=False, node_ids=True, rounded=True)
    plt.show()

  def results(self, show_df = False, show_df_in_latex = False, show_cm = False, show_eval = False):
    """
    Displays the results including confusion matrices and evaluation metrics.

    Args:
        show_df (bool): Whether to display the results DataFrame. Defaults to False.
        show_df_in_latex (bool): Whether to display the results DataFrame in LaTeX format. Defaults to False.
        show_cm (bool): Whether to show the confusion matrix plots. Defaults to False.
        show_eval (bool): Whether to show the evaluation metrics plot. Defaults to False.
    """
    # Execptions
    if not self.predictions:
      self.test_models()

    # Print the results
    train_accuracy = {}
    test_accuracy = {}
    test_sensitivity = {}
    test_specificity = {}
    test_f1 = {}

    if not self.NLP:
      fig, axes = plt.subplots(1, len(self.predictions), figsize=(14, 4))
      fig.suptitle("Confusion Matrices by Model")
    else:
      fig, ax = plt.subplots(1, 1, figsize=(4, 4))

    for idx, (model, predictions) in enumerate(self.predictions.items()):
        predictions_array = np.array(predictions)

        # Calculate the evaluation metrices for each model
        TP = np.sum((predictions_array == 'Cough') & (self.y_test == 'Cough'))
        TN = np.sum((predictions_array == 'Infected wound') & (self.y_test == 'Infected wound'))
        FP = np.sum((predictions_array == 'Cough') & (self.y_test == 'Infected wound'))
        FN = np.sum((predictions_array == 'Infected wound') & (self.y_test == 'Cough'))
        if model == "Decision Tree":
          train_accuracy[model] = round(np.mean(self.best_model[model].predict(self.X_train_unscaled) == self.y_train), 3)
        else:
          train_accuracy[model] = round(np.mean(self.best_model[model].predict(self.X_train) == self.y_train), 3)
        test_accuracy[model] = round(np.mean(predictions_array == self.y_test), 3)
        test_sensitivity[model] = round(TP / (TP + FN), 3) if (TP + FN) > 0 else 0
        test_specificity[model] = round(TN / (TN + FP), 3) if (TN + FP) > 0 else 0
        test_f1[model] = round((2 * TP) / (2 * TP + FP + FN), 3) if (2 * TP + FP + FN) > 0 else 0

        # Calculate the confusion matrix
        cm = confusion_matrix(self.y_test, predictions_array)

        # Plot the confusion matrix
        if not self.NLP:
          sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx], xticklabels=np.unique(self.y_test), yticklabels=np.unique(self.y_test))
          axes[idx].set_title(model)
          axes[idx].set_xlabel('Predicted Label')
          axes[idx].set_ylabel('Actual Label')
        else:
          sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=np.unique(self.y_test), yticklabels=np.unique(self.y_test))
          plt.title(f"Confusion Matrix for {model} in NLP Model")
          plt.xlabel('Predicted Label')
          plt.ylabel('Actual Label')

    df = pd.DataFrame({
        'Model': list(self.predictions.keys()),
        'Best Parameters': list(self.best_params.values()),
        'Train Accuracy': list(train_accuracy.values()),
        'Test Accuracy': list(test_accuracy.values()),
        'Test Sensitivity': list(test_sensitivity.values()),
        'Test Specificity': list(test_specificity.values()),
        'Test F1 Score': list(test_f1.values())})

    if show_df and not show_df_in_latex:
      display(df)
    if show_df and show_df_in_latex:
      # Convert the DataFrame to a LaTeX table
      latex_table = df.to_latex(index=False)
      display(Latex(latex_table))

    if show_cm:
      # Adjust layout and show the confusion matrices plot
      plt.tight_layout()
      plt.show()

    fig, ax = plt.subplots(1, 1, figsize=(3.75*len(self.best_model), 5))
    # Melt the DataFrame to combine Sensitivity and Specificity into a single column
    df_melted = df.drop(columns=['Train Accuracy','Best Parameters']).melt('Model', var_name='Metric', value_name='Value')

    # Plot the bar plot
    ax = sns.barplot(x='Model', y='Value', hue='Metric', data=df_melted)

    # Annotate the bars with their values
    for i in ax.containers:
      ax.bar_label(i,)

    # Set the plot title and labels
    plt.title('Test Set Evaluation Metrics by Model')
    plt.xlabel('Model')
    plt.legend(bbox_to_anchor=(1.05, 1), loc=2)
    if show_eval:
      plt.show()
