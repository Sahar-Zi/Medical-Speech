{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Sahar-Zi/Medical-Speech/blob/main/Classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMt5eCQGe2IF"
   },
   "source": [
    "#**EDA and Modeling Classes**\n",
    "##**EDAClass**\n",
    "The EDAClass is designed for Exploratory Data Analysis (EDA) and feature engineering on datasets involving audio files and associated metadata. This class provides methods to preprocess data, perform various EDA tasks, extract and analyze audio features, and conduct clustering analysis.\n",
    "\n",
    "**Constructor**:\n",
    "\n",
    "* `__init__(self, filepath, audio_dir, binary_outcome=True)`: Initializes the class with a CSV file containing metadata and a directory for audio files. Optionally subsets the data to include only specified categories if binary_outcome is set to True.\n",
    "\n",
    "**Methods**:\n",
    "\n",
    "* `get_data()`: Returns the loaded dataset as a pandas DataFrame.\n",
    "\n",
    "* `EDA_quality_measures()`: Plots frequency, histogram, and density plots for quality measures in the dataset.\n",
    "\n",
    "* `EDA_writers_speakers()`: Provides histograms for speaker and writer IDs.\n",
    "\n",
    "* `EDA_symptoms()`: Plots label frequencies for symptoms.\n",
    "\n",
    "* `Correlation_Matrix()`: Displays a heatmap of the absolute correlations between features.\n",
    "\n",
    "* `NLP(show_wordcloud=True, drop_leading_words=False, inplace=False)`: Performs Natural Language Processing on the 'phrase' column to clean and preprocess text, with options to display a word cloud and/or remove leading words. Updates features in the dataset or returns them based on the inplace parameter.\n",
    "\n",
    "* `Add_audio_features(inplace=False)`: Extracts and adds audio features from audio files to the dataset. Features include MFCCs, spectral centroid, zero crossing rate, and root mean square energy.\n",
    "\n",
    "* `Clustering(random_state=1, n_components=5, n_clusters=2, inplace=False)`: Applies clustering algorithms (KMeans and Gaussian Mixture Models) to the features using PCA and t-SNE for dimensionality reduction. Optionally updates the dataset with cluster labels.\n",
    "\n",
    "---\n",
    "##**modelingClass**\n",
    "The modelingClass is designed to facilitate the machine learning modeling process, encompassing data preparation, model training, testing, and result visualization. This class provides methods to split and scale data, train various machine learning models using grid search for hyperparameter tuning, evaluate model performance, and visualize results including decision tree plots and confusion matrices.\n",
    "\n",
    "**Constructor**:\n",
    "\n",
    "* `__init__(self, X, y, NLP=False)`: Initializes the class with feature dataset X and target labels y. Optionally specifies whether the data pertains to Natural Language Processing (NLP) tasks through the NLP parameter.\n",
    "\n",
    "**Methods**:\n",
    "\n",
    "* `split_scale_data(random_state=1, test_size=0.3)`: Splits the dataset into training and test sets and scales the features. Handles exceptions related to data format and consistency.\n",
    "\n",
    "* `train_models(random_state=1, cv=5)`: Trains various machine learning models (Logistic Regression, K-Nearest Neighbors, Support Vector Machine, Decision Tree) using GridSearchCV to find the best hyperparameters. Handles different setups for NLP and non-NLP tasks.\n",
    "\n",
    "* `test_models()`: Tests the trained models on the test dataset and stores predictions. Ensures that models are trained before testing.\n",
    "\n",
    "* `plot_tree()`: Plots the decision tree model if it has been trained. Raises an error for NLP models as decision trees are not applicable.\n",
    "\n",
    "* `results(show_df=False, show_df_in_latex=False, show_cm=False, show_eval=False)`: Displays the results including confusion matrices and evaluation metrics. Offers options to display results as a DataFrame or in LaTeX format, show confusion matrices, and plot evaluation metrics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9gcPzIhla6M"
   },
   "source": [
    "# **Table of Contents**\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. [Libraries Imports](#SectionImports)\n",
    "\n",
    "#### 2. [Explenatory Data Analysis Class](#SectionEDA)\n",
    "\n",
    "#### 3. [Modeling Class](#SectionModeling)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvZuUQN6Ftwa"
   },
   "source": [
    "# **Libraries Imports**\n",
    "<a class=\"anchor\" id=\"SectionImports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "urhEH-oHCozY",
    "outputId": "9acfb1dd-4872-46b2-b32b-330e4e31e745"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.7.5-cp38-cp38-win_amd64.whl (12.5 MB)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "spyder 4.1.5 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\n",
      "spyder 4.1.5 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\n",
      "rich 13.7.1 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you'll have typing-extensions 3.7.4.3 which is incompatible.\n",
      "cloudpathlib 0.18.1 requires typing_extensions>4; python_version < \"3.11\", but you'll have typing-extensions 3.7.4.3 which is incompatible.\n",
      "annotated-types 0.7.0 requires typing-extensions>=4.0.0; python_version < \"3.9\", but you'll have typing-extensions 3.7.4.3 which is incompatible.\n",
      "pydantic-core 2.20.1 requires typing-extensions!=4.7.0,>=4.6.0, but you'll have typing-extensions 3.7.4.3 which is incompatible.\n",
      "pydantic 2.8.2 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you'll have typing-extensions 3.7.4.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.10-cp38-cp38-win_amd64.whl (25 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.9-cp38-cp38-win_amd64.whl (122 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\97252\\anaconda3\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.8-cp38-cp38-win_amd64.whl (483 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.8-cp38-cp38-win_amd64.whl (39 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\97252\\anaconda3\\lib\\site-packages (from spacy) (50.3.1.post20201107)\n",
      "Requirement already satisfied: numpy>=1.15.0; python_version < \"3.9\" in c:\\users\\97252\\anaconda3\\lib\\site-packages (from spacy) (1.19.2)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\97252\\anaconda3\\lib\\site-packages (from spacy) (20.4)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\97252\\anaconda3\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\97252\\anaconda3\\lib\\site-packages (from spacy) (4.50.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2\n",
      "  Using cached thinc-8.2.5.tar.gz (193 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting rich>=10.11.0\n",
      "  Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\97252\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\97252\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\97252\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\97252\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\97252\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Collecting colorama>=0.4.6; sys_platform == \"win32\" and python_version >= \"3.7\"\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Using cached cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Collecting confection<0.2.0,>=0.0.4\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\97252\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\97252\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\97252\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\97252\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.20.1\n",
      "  Downloading pydantic_core-2.20.1-cp38-none-win_amd64.whl (1.9 MB)\n",
      "Collecting language-data>=1.2\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.11-cp38-cp38-win_amd64.whl (6.6 MB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\97252\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.11.2)\n",
      "Collecting marisa-trie>=0.7.7\n",
      "  Downloading marisa_trie-1.2.0-cp38-cp38-win_amd64.whl (152 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: thinc\n",
      "  Building wheel for thinc (PEP 517): started\n",
      "  Building wheel for thinc (PEP 517): finished with status 'done'\n",
      "  Created wheel for thinc: filename=thinc-8.2.5-cp38-cp38-win_amd64.whl size=1488050 sha256=8522b357fe8963d3f6029ccffd09495b9114c7924e11a4a3bf955f72ca74fa51\n",
      "  Stored in directory: c:\\users\\97252\\appdata\\local\\pip\\cache\\wheels\\ca\\65\\2f\\a856a5c5a2338dcc54a8e8fbbd904aef94510ff7575effd769\n",
      "Successfully built thinc\n",
      "Installing collected packages: murmurhash, shellingham, mdurl, markdown-it-py, pygments, rich, typer, cymem, preshed, catalogue, srsly, spacy-loggers, colorama, wasabi, cloudpathlib, smart-open, annotated-types, pydantic-core, pydantic, confection, weasel, marisa-trie, language-data, langcodes, spacy-legacy, blis, thinc, spacy\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.7.2\n",
      "    Uninstalling Pygments-2.7.2:\n",
      "      Successfully uninstalled Pygments-2.7.2\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 colorama-0.4.6 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.8.2 pydantic-core-2.20.1 pygments-2.18.0 rich-13.7.1 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.3 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cPIIIizwFQ6F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "# Text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "# Audio\n",
    "import librosa\n",
    "import os\n",
    "import IPython\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XCnMEelFwNn"
   },
   "source": [
    "# **EDA and Feature Engineering**\n",
    "<a class=\"anchor\" id=\"SectionEDA\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XmQ6E-tb9hAG"
   },
   "outputs": [],
   "source": [
    "class EDAClass:\n",
    "  def __init__(self, filepath, audio_dir, binary_outcome = True):\n",
    "    \"\"\"\n",
    "    Initializes the EDAObject with the given file path and audio directory.\n",
    "\n",
    "    Args:\n",
    "      filepath (str): Path to the CSV file containing the data.\n",
    "      audio_dir (str): Directory where audio files are stored.\n",
    "      binary_outcome (bool): If True, subset the data to only include \"Cough\" and \"Infected wound\".\n",
    "    \"\"\"\n",
    "    self.audio_dir = audio_dir\n",
    "    self.data = pd.read_csv(filepath)\n",
    "\n",
    "    self.data.rename(columns={'prompt':'Symptoms'}, inplace=True)\n",
    "    # Subset for binary outcome: \"Cough\" or \"Infected Wound\"\n",
    "    if binary_outcome:\n",
    "      self.data = self.data[((self.data['Symptoms'] == \"Cough\") | (self.data['Symptoms'] == \"Infected wound\"))]\n",
    "      self.data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Find and convert features to category type\n",
    "    category_features = self.data.select_dtypes(include=['object']).columns.append(self.data.select_dtypes(include=['int64']).columns)\n",
    "    self.data[category_features] = self.data[category_features].astype('category')\n",
    "    self.data['phrase'] = self.data['phrase'].astype('string')\n",
    "    self.data['file_name'] = self.data['file_name'].astype('string')\n",
    "\n",
    "    self.X = self.data.select_dtypes(include=['float64'])\n",
    "    self.y = self.data['Symptoms']\n",
    "\n",
    "  def get_data(self):\n",
    "    \"\"\"\n",
    "    Returns the data loaded into the EDAObject.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    return self.data\n",
    "\n",
    "  def EDA_quality_measures(self):\n",
    "    \"\"\"\n",
    "    Performs Exploratory Data Analysis (EDA) on quality measures.\n",
    "    Plots frequency, histogram, and density plots for audio quality measures.\n",
    "    \"\"\"\n",
    "    quality_measures = ['audio_clipping', 'quiet_speaker', 'background_noise_audible']\n",
    "\n",
    "    fig, axes = plt.subplots(3, len(quality_measures), figsize=(18, 12), gridspec_kw={'height_ratios': [0.6, 3, 3]})\n",
    "\n",
    "    for i, column in enumerate(quality_measures):\n",
    "        sns.heatmap(self.data[column].value_counts().to_frame().T,\n",
    "                    ax=axes[0, i], annot=True, fmt=\"d\", cmap=\"Set3\", cbar=False, yticklabels=False)\n",
    "        axes[0, i].xaxis.tick_top()\n",
    "        axes[0, i].xaxis.set_label_position('top')\n",
    "        axes[0, i].set_title(column.capitalize(), fontsize=16)\n",
    "        axes[0, i].set(xlabel=\"\")\n",
    "        sns.histplot(data=self.data, ax=axes[1, i], x=column+\":confidence\", fill=True)\n",
    "        axes[1, i].set_xlim(0, 1.2)\n",
    "        axes[1, i].set_ylim(0, 600)\n",
    "        sns.kdeplot(data=self.data, ax=axes[2, i], x=column+\":confidence\", hue=column, palette=\"tab10\", fill=True, warn_singular=False)\n",
    "        axes[2, i].set_xlim(0, 1.2)\n",
    "        sns.move_legend(axes[2, i], \"upper left\")\n",
    "\n",
    "    fig.suptitle(\"Frequency, Histogram, and Density of Quality Measures\", y=0.97, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "    sns.histplot(data=self.data, ax=axes[0], x=\"overall_quality_of_the_audio\", fill=True, bins=6)\n",
    "    sns.kdeplot(data=self.data, ax=axes[1], x=\"overall_quality_of_the_audio\", hue=\"Symptoms\", fill=True, warn_singular=False)\n",
    "    axes[0].set_xlim(0, 5)\n",
    "    axes[1].set_xlim(0, 5)\n",
    "    sns.move_legend(axes[1], \"upper left\")\n",
    "\n",
    "    fig.suptitle(\"Histogram, and Density of the Overall Quality Measure\", y=0.97, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "  def EDA_writers_speakers(self):\n",
    "    \"\"\"\n",
    "    Performs Exploratory Data Analysis (EDA) on writer and speaker IDs.\n",
    "    Plots histograms for speaker and writer IDs.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    self.data['speaker_id'].value_counts().plot(kind='bar')\n",
    "    plt.title('Speaker ID Histogram')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    self.data['writer_id'].value_counts().plot(kind='bar')\n",
    "    plt.title('Writer ID Histogram')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "  def EDA_symptoms(self):\n",
    "    \"\"\"\n",
    "    Performs Exploratory Data Analysis (EDA) on symptoms.\n",
    "    Plots label frequencies of symptoms.\n",
    "    \"\"\"\n",
    "    x = self.data['Symptoms'].nunique()\n",
    "    plt.figure(figsize=(0.34782608695652*x + 3.304347826087, 0.17391304347826*x + 1.6521739130435))\n",
    "    ax = sns.countplot(y=\"Symptoms\", data=self.data, order=self.data['Symptoms'].value_counts().index)\n",
    "    plt.title(\"Label Frequencies\")\n",
    "    for p in ax.patches:\n",
    "      ax.annotate(int(p.get_width()), (p.get_width()+1.2608695652174*x - 42.521739130435, p.get_y()+0.0047826086956522*x + 0.4404347826087),color=\"white\",fontsize=-0.17391304347826*x + 12.347826086957)\n",
    "    plt.show()\n",
    "\n",
    "  def Correlation_Matrix(self):\n",
    "    \"\"\"\n",
    "    Plots the absolute correlation heatmap for the features.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(self.X.shape[1], self.X.shape[1]))\n",
    "    sns.heatmap(self.X.corr().abs(), cmap='YlOrRd', annot=True)\n",
    "    plt.title('Absolute Correlation Heatmap')\n",
    "    plt.show()\n",
    "\n",
    "  def NLP(self, show_wordcloud = True, drop_leading_words = False, inplace = False):\n",
    "    \"\"\"\n",
    "    Performs NLP preprocessing on the 'phrase' column.\n",
    "    Optionally displays a word cloud and drops leading words.\n",
    "\n",
    "    Args:\n",
    "      show_wordcloud (bool): If True, displays a word cloud.\n",
    "      drop_leading_words (bool): If True, drops leading words related to symptoms.\n",
    "      inplace (bool): If True, updates the X attribute of the class with new features.\n",
    "    \"\"\"\n",
    "    def abv_decontraction(phrase):\n",
    "      # specific\n",
    "      phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "      phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "      # general\n",
    "      phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "      phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "      phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "      phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "      phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "      phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "      phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "      phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "      return phrase\n",
    "\n",
    "    # Abbreviations deconstraction\n",
    "    phrases = self.data['phrase'].apply(lambda x: abv_decontraction(x))\n",
    "\n",
    "    # Lower case all letters\n",
    "    phrases = phrases.apply(lambda x:x.lower())\n",
    "\n",
    "    # Remove Special characters\n",
    "    phrases = phrases.str.replace('\\W', ' ')\n",
    "\n",
    "    # Remove stop words\n",
    "    STOPWORDS = stopwords.words('english')\n",
    "    phrases = phrases.apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS]))\n",
    "\n",
    "    # Lemmatization\n",
    "    lem = WordNetLemmatizer()\n",
    "    phrases = phrases.apply(lambda x: ' '.join([lem.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    # Remove special charachters\n",
    "    alphabet = list(string.ascii_lowercase)\n",
    "    phrases = phrases.apply(lambda x: ' '.join([word for word in x.split() if all(map(lambda letter: letter in alphabet, word))]))\n",
    "\n",
    "    if show_wordcloud:\n",
    "      word_dict = dict(Counter([word for phrase in phrases for word in phrase.split()]))\n",
    "      popular_words = sorted(word_dict, key = word_dict.get, reverse = True)\n",
    "      word_string=\" \".join(popular_words)\n",
    "      wordcloud = WordCloud(background_color='white',\n",
    "                            colormap='plasma',\n",
    "                            max_words=len(word_dict),\n",
    "                            width=1920,height=1080).generate(word_string)\n",
    "      plt.clf()\n",
    "      plt.imshow(wordcloud)\n",
    "      plt.axis('off')\n",
    "      plt.show()\n",
    "\n",
    "      phrases_df = phrases.str.get_dummies(sep=' ')\n",
    "\n",
    "      if drop_leading_words:\n",
    "        phrases_df.drop(['cough', 'coughing','infected','infection','wound'], axis=1, inplace = True)\n",
    "\n",
    "      if inplace:\n",
    "        self.X = self.X.join(phrases_df)\n",
    "      else:\n",
    "        return self.X.join(phrases_df)\n",
    "\n",
    "  def Add_audio_features(self, inplace = False):\n",
    "    \"\"\"\n",
    "    Adds audio features extracted from audio files.\n",
    "\n",
    "    Args:\n",
    "      inplace (bool): If True, updates the X attribute of the class with new features.\n",
    "    \"\"\"\n",
    "    def extract_features(filename):\n",
    "      \"\"\"\n",
    "      Extracts audio features from a given audio file.\n",
    "\n",
    "      Args:\n",
    "        filename (str): Name of the audio file.\n",
    "\n",
    "      Returns:\n",
    "        dict: Dictionary of extracted audio features.\n",
    "      \"\"\"\n",
    "      # Load audio file\n",
    "      file_path = os.path.join(self.audio_dir, filename)\n",
    "      y, sr = librosa.load(file_path)\n",
    "      duration = librosa.get_duration(y=y, sr=sr)\n",
    "      mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "      mfcc_means = np.mean(mfccs, axis=1)\n",
    "      mfcc_stds = np.std(mfccs, axis=1)\n",
    "      spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "      zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
    "      rmse = librosa.feature.rms(y=y)\n",
    "\n",
    "      # Aggregate features\n",
    "      features = {\n",
    "          'duration': duration,\n",
    "          'spectral_centroid_mean': np.mean(spectral_centroid),\n",
    "          'spectral_centroid_std': np.std(spectral_centroid),\n",
    "          'zero_crossing_rate_mean': np.mean(zero_crossing_rate),\n",
    "          'zero_crossing_rate_std': np.std(zero_crossing_rate),\n",
    "          'rms_energy_mean': np.mean(rmse),\n",
    "          'rms_energy_std': np.std(rmse)\n",
    "      }\n",
    "      features.update({f'mfcc_{i+1}': mfcc_means[i]/mfcc_stds[i] for i in range(13)})\n",
    "      return features\n",
    "\n",
    "    audio_features = pd.DataFrame(self.data['file_name'].apply(extract_features).tolist())\n",
    "\n",
    "    if inplace:\n",
    "      self.X = self.X.join(audio_features)\n",
    "    else:\n",
    "      return self.X.join(audio_features)\n",
    "\n",
    "  def Clustering(self, random_state = 1, n_components = 5, n_clusters = 2, inplace = False):\n",
    "    \"\"\"\n",
    "    Performs clustering on the features using PCA, t-SNE, KMeans, and Gaussian Mixture Models.\n",
    "\n",
    "    Args:\n",
    "      random_state (int): Random state for reproducibility.\n",
    "      n_components (int): Number of components for PCA and t-SNE.\n",
    "      n_clusters (int): Number of clusters for KMeans and GMM.\n",
    "      inplace (bool): If True, updates the X attribute of the class with cluster labels.\n",
    "    \"\"\"\n",
    "    X_scaled = pd.DataFrame(StandardScaler().fit_transform(self.X), columns=self.X.columns)\n",
    "    PCA_clustered_data = pd.DataFrame(PCA(n_components=n_components).fit_transform(X_scaled), columns=[f'PC {i+1}' for i in range(n_components)])\n",
    "    TSNE_clustered_data = pd.DataFrame(TSNE(n_components=n_components, random_state=random_state, init='random', method=\"exact\", perplexity=2).fit_transform(X_scaled), columns=[f'TSNE {i+1}' for i in range(n_components)])\n",
    "\n",
    "    km_method = pd.Series(KMeans(random_state = random_state, n_clusters = n_clusters,n_init='auto').fit(X_scaled).labels_)\n",
    "    PCA_clustered_data['clusters_km'] = km_method\n",
    "    TSNE_clustered_data['clusters_km'] = km_method\n",
    "\n",
    "    gmm_method = pd.Series(GaussianMixture(random_state = random_state, n_components = n_clusters).fit_predict(X_scaled))\n",
    "    PCA_clustered_data['clusters_gmm'] = gmm_method\n",
    "    TSNE_clustered_data['clusters_gmm'] = gmm_method\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
    "    sns.scatterplot(x=PCA_clustered_data['PC 1'], y=PCA_clustered_data['PC 2'], hue=PCA_clustered_data['clusters_km'], s=100, ax=axes[0,0])\n",
    "    sns.scatterplot(x=TSNE_clustered_data['TSNE 1'], y=TSNE_clustered_data['TSNE 2'], hue=TSNE_clustered_data['clusters_km'], s=100, ax=axes[0,1])\n",
    "    sns.scatterplot(x=PCA_clustered_data['PC 1'], y=PCA_clustered_data['PC 2'], hue=PCA_clustered_data['clusters_gmm'], s=100, ax=axes[1,0])\n",
    "    sns.scatterplot(x=TSNE_clustered_data['TSNE 1'], y=TSNE_clustered_data['TSNE 2'], hue=TSNE_clustered_data['clusters_gmm'], s=100, ax=axes[1,1])\n",
    "    axes[0,0].set_title('PC 1 against PC 2 for K-means')\n",
    "    axes[0,1].set_title('TSNE 1 against TSNE 2 for K-means')\n",
    "    axes[1,0].set_title('PC 1 against PC 2 for Gaussian Mixture Model')\n",
    "    axes[1,1].set_title('TSNE 1 against TSNE 2 for Gaussian Mixture Model')\n",
    "    axes[0,0].grid()\n",
    "    axes[0,1].grid()\n",
    "    axes[1,0].grid()\n",
    "    axes[1,1].grid()\n",
    "    plt.show()\n",
    "\n",
    "    if inplace:\n",
    "      self.X = self.X.join(PCA_clustered_data['clusters_gmm'])\n",
    "    else:\n",
    "      return self.X.join(PCA_clustered_data['clusters_gmm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UNpVRSGVhwp"
   },
   "source": [
    "# **Modeling**\n",
    "<a class=\"anchor\" id=\"SectionModeling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3SePN1SRVhwq"
   },
   "outputs": [],
   "source": [
    "class modelingClass:\n",
    "  \"\"\"\n",
    "  A class to encapsulate the machine learning modeling process including data splitting, scaling,\n",
    "  training various models, testing, and visualizing results.\n",
    "\n",
    "  Attributes:\n",
    "      X (DataFrame or ndarray): Features dataset.\n",
    "      y (Series or ndarray): Target labels.\n",
    "      NLP (bool): Whether the data is for NLP tasks.\n",
    "      X_train (ndarray): Training set features.\n",
    "      X_test (ndarray): Test set features.\n",
    "      X_train_unscaled (ndarray): Unscaled training set features.\n",
    "      X_test_unscaled (ndarray): Unscaled test set features.\n",
    "      y_train (ndarray): Training set labels.\n",
    "      y_test (ndarray): Test set labels.\n",
    "      best_model (dict): Dictionary to store the best model for each algorithm.\n",
    "      best_params (dict): Dictionary to store the best parameters for each model.\n",
    "      predictions (dict): Dictionary to store predictions for each model.\n",
    "  \"\"\"\n",
    "  def __init__(self, X, y, NLP = False):\n",
    "    \"\"\"\n",
    "    Initializes the modelingObject with the dataset and target labels.\n",
    "\n",
    "    Args:\n",
    "        X (DataFrame or ndarray): Features dataset.\n",
    "        y (Series or ndarray): Target labels.\n",
    "        NLP (bool): Whether the data is for NLP tasks. Defaults to False.\n",
    "    \"\"\"\n",
    "    self.X = X\n",
    "    if type(y) == pd.core.series.Series:\n",
    "      self.y = y.values\n",
    "    else:\n",
    "      self.y = y\n",
    "    self.NLP = NLP\n",
    "    self.X_train = None\n",
    "    self.X_test = None\n",
    "    self.X_train_unscaled = None\n",
    "    self.X_test_unscaled = None\n",
    "    self.y_train = None\n",
    "    self.y_test = None\n",
    "    self.best_model = {}\n",
    "    self.best_params = {}\n",
    "    self.predictions = {}\n",
    "\n",
    "  def split_scale_data(self, random_state=1, test_size=0.3):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and test sets and scales the features.\n",
    "\n",
    "    Args:\n",
    "        random_state (int): Random state for reproducibility. Defaults to 1.\n",
    "        test_size (float): Proportion of the dataset to include in the test split. Defaults to 0.3.\n",
    "    \"\"\"\n",
    "    # Execptions\n",
    "    if self.X is None or (type(self.X) != pd.core.frame.DataFrame and type(self.X) != np.ndarray):\n",
    "      raise ValueError(\"X must be a pandas DataFrame or numpy array\")\n",
    "    if self.y is None or (type(self.y) != pd.core.series.Series and type(self.y) != np.ndarray):\n",
    "      raise ValueError(\"y must be a pandas Series or numpy array\")\n",
    "    if len(self.y.shape) != 1 :\n",
    "      raise ValueError(\"y must have only one column\")\n",
    "    if self.X.shape[0] != self.y.shape[0]:\n",
    "      raise ValueError(\"X and y must have the same number of rows\")\n",
    "    if self.X.shape[0] < 2 or y.shape[0] < 2:\n",
    "      raise ValueError(\"X and y must have at least 2 rows\")\n",
    "\n",
    "    # Split the data into train and test sets and scale it\n",
    "    print(\"Splitting and scaling the data\")\n",
    "    print(\"-----------------------------------\")\n",
    "    if self.X_train is None:\n",
    "      self.X_train_unscaled, self.X_test_unscaled, self.y_train, self. y_test = train_test_split(self.X, self.y, random_state=random_state, test_size=test_size)\n",
    "      scaler = StandardScaler().fit(self.X_train_unscaled)\n",
    "      self.X_train = scaler.transform(self.X_train_unscaled)\n",
    "      self.X_test = scaler.transform(self.X_test_unscaled)\n",
    "\n",
    "\n",
    "  def train_models(self, random_state=1, cv=5):\n",
    "    \"\"\"\n",
    "    Trains various machine learning models using GridSearchCV to find the best parameters.\n",
    "\n",
    "    Args:\n",
    "        random_state (int): Random state for reproducibility. Defaults to 1.\n",
    "        cv (int): Number of cross-validation folds. Defaults to 5.\n",
    "    \"\"\"\n",
    "    # Exceptions\n",
    "    if self.X_train is None:\n",
    "      self.split_scale_data(random_state = random_state)\n",
    "\n",
    "    # Initialization of the models\n",
    "    init_LR = LogisticRegression(random_state=random_state, solver = 'saga')  # For logistic regression with regularization one must choose 'saga' solver\n",
    "    if not self.NLP:\n",
    "      init_KNN = KNeighborsClassifier()\n",
    "      init_SVM = SVC(random_state=random_state, probability=True)\n",
    "      init_DT = DecisionTreeClassifier(random_state=random_state)\n",
    "\n",
    "    # Setting grid search parameter grids'\n",
    "    parameters_LR = {'penalty':['l1','l2'], # L1 penalty = Lasso, L2 penalty = Ridge\n",
    "                      'C': np.exp(np.arange(-1,0,0.05))} # small C values means stronger regularization and the opposite\n",
    "    if not self.NLP:\n",
    "      parameters_KNN = {'n_neighbors': range(3, self.X_test.shape[0], 2)}\n",
    "      parameters_SVM = {'C': np.exp(np.arange(-1,0,0.1)),\n",
    "                        'kernel': ['linear', 'poly', 'rbf'],\n",
    "                        'degree': np.arange(3,6,1),\n",
    "                        'gamma': 10**np.arange(-4, 0, 1, dtype=np.float64)}\n",
    "      parameters_DT = {'max_depth': np.arange(3,7,1),\n",
    "                        'min_samples_leaf': np.arange(5,8,1),\n",
    "                        'criterion': ['gini','entropy']}\n",
    "\n",
    "    # Cross validation and finding the most 'accurate' model for each set of parameters\n",
    "    print(\"Training Phase:\")\n",
    "    print(\"Training \\'Logistic Regression\\' model\")\n",
    "    GSCV = GridSearchCV(init_LR, parameters_LR, cv=cv, n_jobs=-1, scoring='accuracy', verbose=3).fit(self.X_train, self.y_train)\n",
    "    self.best_model[\"Logistic Regression\"] = GSCV.best_estimator_\n",
    "    self.best_params[\"Logistic Regression\"] = GSCV.best_params_\n",
    "    if not self.NLP:\n",
    "      print(\"Training \\'K-Nearest Neighbors\\' model\")\n",
    "      GSCV = GridSearchCV(init_KNN, parameters_KNN, cv=cv, n_jobs=-1, scoring='accuracy', verbose=3).fit(self.X_train, self.y_train)\n",
    "      self.best_model[\"K-Nearest Neighbors\"] = GSCV.best_estimator_\n",
    "      self.best_params[\"K-Nearest Neighbors\"] = GSCV.best_params_\n",
    "      print(\"Training \\'Support Vector Machine\\' model\")\n",
    "      GSCV = GridSearchCV(init_SVM, parameters_SVM, cv=cv, n_jobs=-1, verbose=3).fit(self.X_train, self.y_train)\n",
    "      self.best_model[\"Support Vector Machine\"] = GSCV.best_estimator_\n",
    "      self.best_params[\"Support Vector Machine\"] = GSCV.best_params_\n",
    "      print(\"Training \\'Decision Tree\\' model\")\n",
    "      GSCV = GridSearchCV(init_DT, parameters_DT, cv=cv, n_jobs=-1, scoring='accuracy', verbose=3).fit(self.X_train_unscaled, self.y_train)\n",
    "      self.best_model[\"Decision Tree\"] = GSCV.best_estimator_\n",
    "      self.best_params[\"Decision Tree\"] = GSCV.best_params_\n",
    "    print(\"The training of the models is done.\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "  def test_models(self):\n",
    "    \"\"\"\n",
    "    Tests the trained models on the test dataset and stores the predictions.\n",
    "    \"\"\"\n",
    "    # Exceptions\n",
    "    if not self.best_model:\n",
    "      self.train_models()\n",
    "\n",
    "    # Testing the models\n",
    "    print(\"Testing Phase:\")\n",
    "    print(\"Testing \\'Logistic Regression\\' model\")\n",
    "    self.predictions[\"Logistic Regression\"] = self.best_model[\"Logistic Regression\"].predict(self.X_test)\n",
    "    if not self.NLP:\n",
    "      print(\"Testing \\'K-Nearest Neighbors\\' model\")\n",
    "      self.predictions[\"K-Nearest Neighbors\"] = self.best_model[\"K-Nearest Neighbors\"].predict(self.X_test)\n",
    "      print(\"Testing \\'Support Vector Machine\\' model\")\n",
    "      self.predictions[\"Support Vector Machine\"] = self.best_model[\"Support Vector Machine\"].predict(self.X_test)\n",
    "      print(\"Testing \\'Decision Tree\\' model\")\n",
    "      self.predictions[\"Decision Tree\"] = self.best_model[\"Decision Tree\"].predict(self.X_test_unscaled)\n",
    "    print(\"The testing of the models is done.\")\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "  def plot_tree(self):\n",
    "    \"\"\"\n",
    "    Plots the decision tree model if it's included in the trained models.\n",
    "    \"\"\"\n",
    "    # Execptions\n",
    "    if self.NLP:\n",
    "      raise ValueError(\"This method is not for NLP models\")\n",
    "    if not self.predictions:\n",
    "      self.test_models()\n",
    "\n",
    "    # Plot the decision tree fitted over the train set\n",
    "    d = self.best_model[\"Decision Tree\"].get_depth()\n",
    "    plt.figure(figsize=(6*(d+1),3*(d+1)))\n",
    "    tree.plot_tree(self.best_model[\"Decision Tree\"].fit(self.X_train_unscaled, self.y_train), fontsize=15, filled=True, feature_names=self.X.columns, impurity=False, node_ids=True, rounded=True)\n",
    "    plt.show()\n",
    "\n",
    "  def results(self, show_df = False, show_df_in_latex = False, show_cm = False, show_eval = False):\n",
    "    \"\"\"\n",
    "    Displays the results including confusion matrices and evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        show_df (bool): Whether to display the results DataFrame. Defaults to False.\n",
    "        show_df_in_latex (bool): Whether to display the results DataFrame in LaTeX format. Defaults to False.\n",
    "        show_cm (bool): Whether to show the confusion matrix plots. Defaults to False.\n",
    "        show_eval (bool): Whether to show the evaluation metrics plot. Defaults to False.\n",
    "    \"\"\"\n",
    "    # Execptions\n",
    "    if not self.predictions:\n",
    "      self.test_models()\n",
    "\n",
    "    # Print the results\n",
    "    train_accuracy = {}\n",
    "    test_accuracy = {}\n",
    "    test_sensitivity = {}\n",
    "    test_specificity = {}\n",
    "    test_f1 = {}\n",
    "\n",
    "    if not self.NLP:\n",
    "      fig, axes = plt.subplots(1, len(self.predictions), figsize=(14, 4))\n",
    "      fig.suptitle(\"Confusion Matrices by Model\")\n",
    "    else:\n",
    "      fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "    for idx, (model, predictions) in enumerate(self.predictions.items()):\n",
    "        predictions_array = np.array(predictions)\n",
    "\n",
    "        # Calculate the evaluation metrices for each model\n",
    "        TP = np.sum((predictions_array == 'Cough') & (self.y_test == 'Cough'))\n",
    "        TN = np.sum((predictions_array == 'Infected wound') & (self.y_test == 'Infected wound'))\n",
    "        FP = np.sum((predictions_array == 'Cough') & (self.y_test == 'Infected wound'))\n",
    "        FN = np.sum((predictions_array == 'Infected wound') & (self.y_test == 'Cough'))\n",
    "        if model == \"Decision Tree\":\n",
    "          train_accuracy[model] = round(np.mean(self.best_model[model].predict(self.X_train_unscaled) == self.y_train), 3)\n",
    "        else:\n",
    "          train_accuracy[model] = round(np.mean(self.best_model[model].predict(self.X_train) == self.y_train), 3)\n",
    "        test_accuracy[model] = round(np.mean(predictions_array == self.y_test), 3)\n",
    "        test_sensitivity[model] = round(TP / (TP + FN), 3) if (TP + FN) > 0 else 0\n",
    "        test_specificity[model] = round(TN / (TN + FP), 3) if (TN + FP) > 0 else 0\n",
    "        test_f1[model] = round((2 * TP) / (2 * TP + FP + FN), 3) if (2 * TP + FP + FN) > 0 else 0\n",
    "\n",
    "        # Calculate the confusion matrix\n",
    "        cm = confusion_matrix(self.y_test, predictions_array)\n",
    "\n",
    "        # Plot the confusion matrix\n",
    "        if not self.NLP:\n",
    "          sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx], xticklabels=np.unique(self.y_test), yticklabels=np.unique(self.y_test))\n",
    "          axes[idx].set_title(model)\n",
    "          axes[idx].set_xlabel('Predicted Label')\n",
    "          axes[idx].set_ylabel('Actual Label')\n",
    "        else:\n",
    "          sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=np.unique(self.y_test), yticklabels=np.unique(self.y_test))\n",
    "          plt.title(f\"Confusion Matrix for {model} in NLP Model\")\n",
    "          plt.xlabel('Predicted Label')\n",
    "          plt.ylabel('Actual Label')\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Model': list(self.predictions.keys()),\n",
    "        'Best Parameters': list(self.best_params.values()),\n",
    "        'Train Accuracy': list(train_accuracy.values()),\n",
    "        'Test Accuracy': list(test_accuracy.values()),\n",
    "        'Test Sensitivity': list(test_sensitivity.values()),\n",
    "        'Test Specificity': list(test_specificity.values()),\n",
    "        'Test F1 Score': list(test_f1.values())})\n",
    "\n",
    "    if show_df and not show_df_in_latex:\n",
    "      display(df)\n",
    "    if show_df and show_df_in_latex:\n",
    "      # Convert the DataFrame to a LaTeX table\n",
    "      latex_table = df.to_latex(index=False)\n",
    "      display(Latex(latex_table))\n",
    "\n",
    "    if show_cm:\n",
    "      # Adjust layout and show the confusion matrices plot\n",
    "      plt.tight_layout()\n",
    "      plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3.75*len(self.best_model), 5))\n",
    "    # Melt the DataFrame to combine Sensitivity and Specificity into a single column\n",
    "    df_melted = df.drop(columns=['Train Accuracy','Best Parameters']).melt('Model', var_name='Metric', value_name='Value')\n",
    "\n",
    "    # Plot the bar plot\n",
    "    ax = sns.barplot(x='Model', y='Value', hue='Metric', data=df_melted)\n",
    "\n",
    "    # Annotate the bars with their values\n",
    "    for i in ax.containers:\n",
    "      ax.bar_label(i,)\n",
    "\n",
    "    # Set the plot title and labels\n",
    "    plt.title('Test Set Evaluation Metrics by Model')\n",
    "    plt.xlabel('Model')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2)\n",
    "    if show_eval:\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM0pdHyIVWnEVtvfwnE6/ig",
   "collapsed_sections": [
    "uvZuUQN6Ftwa",
    "-XCnMEelFwNn",
    "-UNpVRSGVhwp"
   ],
   "include_colab_link": true,
   "mount_file_id": "1DbL3LQhNgfaIRHCxsgcz2nPWJIVZUmw3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
